student:
  model_name: stdc2
  student_keys: [res5, res4]
  checkpoint_path: /home/arda/dinov2/distillation/checkpoints/STDCNet2.pth




teacher:
  model_name: dinov2_vits14



data_transform:
  n_global_crops: 2
  n_local_crops: 8
  global_crops_scale: [0.08, 1.0]
  local_crops_scale: [0.05, 0.32]
  global_crops_size: [224, 224]
  local_crops_size: [224, 224] 



# new_lr = base_lr * (new_batch_size / base_batch_size)
#base 3e-5

optimizer:
  type: AdamW
  kwargs:
    lr: 1e-3
    betas: [0.9, 0.999]
    weight_decay: 0.01
  scheduler:
    type: CosineAnnealingLR
    kwargs:
      T_max: 240
      eta_min: 1e-5
    monitor: val_loss
    interval: epoch
    frequency: 1

loss:
  losses:
    # - type: scalekd
    #   weight: 1
    #   kwargs:
    #     alpha: [0.08, 0.06]
    #     window_shapes: [1, 1]
    #     self_query: True
    #     softmax_scale: [5.0, 5.0]
    #     dis_freq: high
    #     num_heads: 8
    #     name: scalekd_res2
    #     use_this: true
    # - type: scalekd
    #   weight: 1
    #   kwargs:
    #     alpha: [0.08, 0.06]
    #     window_shapes: [1, 1]
    #     self_query: False
    #     softmax_scale: [5.0, 5.0]
    #     dis_freq: high
    #     num_heads: 12
    #     name: scalekd_res3
    #     use_this: true
    - type: scalekd
      weight: 1
      kwargs:
        alpha: [0.08, 0.06]
        window_shapes: [1, 1]
        self_query: True
        softmax_scale: [5.0, 5.0]
        dis_freq: high
        num_heads: 16
        name: scalekd_res4
        use_this: true
    - type: scalekd
      weight: 1.0
      kwargs:
        alpha: [0.08, 0.06]
        window_shapes: [1, 1]
        self_query: False
        softmax_scale: [5.0, 5.0]
        dis_freq: high
        num_heads: 24
        name: scalekd_res5
        use_this: true


train:
  name: stdc2
  max_epochs: 240
  accelerator: gpu
  devices: [0,1]
  num_nodes: 1
  strategy: ddp
  # resume_from_checkpoint: /home/arda/dinov2/distillation/logs/stdc2/distillation/version_229/checkpoints/last.ckpt  # Add this line
  accumulate_grad_batches: 2  # Accumulate gradients over 4 batches

  
data_loader:
  data_dir: /home/arda/data/train2017
  #val_dir: 
  batch_size: 128 #per gpu
  num_workers: 8
  



checkpoints:
  dirpath: checkpoints
  monitor:  val_scalekd_res5_spatial_similarity
  mode: max
  save_top_k: 1

wandb:
  project: "distillation"  # Project name in W&B
  name: stdc2  # Run name (will use train.name if null)
  tags: ["distillation", "stdc", "dinov2"]  # Searchable tags
  notes: "Knowledge distillation from DINOv2 to stdc"  # Run description
