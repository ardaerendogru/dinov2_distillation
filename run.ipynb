{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mStarting DistillationTrainer initialization...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mStarting config handling...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mConfig handled and updated.\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mCreating data transform...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mData transform created: <dinov2.data.augmentations.DataAugmentationDINO object at 0x76f321763400>\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mCreating data module...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mData module created: <datasets.CustomDataset.CustomDataModule object at 0x76f313039060>\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mCreating teacher and student models...\n",
      "Using cache found in /home/arda/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0musing MLP layer as FFN\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.32M/5.32M [00:00<00:00, 40.1MB/s]\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mTeacher model created: dinov2_vits14\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mStudent model created: edgenext_xx_small\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mCreating distillation module...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mStarting DistillationModule initialization...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mInitializing models...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mFreezing teacher model...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mTeacher model frozen.\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mModels initialized.\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mInitializing loss functions...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mLoss functions initialized: ['scalekd_res4', 'scalekd_res5']\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mDistillationModule initialized.\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mDistillation module created.\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mCreating PyTorch Lightning Trainer...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mardaerendogru2632\u001b[0m (\u001b[33mardaerendogru2632-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/disk0/arda/dinov2_finalize/dinov2/distillation/wandb/run-20250313_154227-fp4njlqj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33medgenext_xx_small_dinov2_vits14\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ardaerendogru2632-/distillation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ardaerendogru2632-/distillation/runs/fp4njlqj\u001b[0m\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mTrainer created: <lightning.pytorch.trainer.trainer.Trainer object at 0x76f312984370>\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mDistillationTrainer initialized.\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mStarting training process...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mStarting training from scratch.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "/home/arda/miniconda3/envs/distillation/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mStarting DistillationTrainer initialization...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mStarting config handling...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mConfig handled and updated.\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mCreating data transform...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mData transform created: <dinov2.data.augmentations.DataAugmentationDINO object at 0x7deb6b763520>\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mCreating data module...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mData module created: <datasets.CustomDataset.CustomDataModule object at 0x7deb5d03cee0>\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mCreating teacher and student models...\n",
      "Using cache found in /home/arda/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0musing MLP layer as FFN\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mTeacher model created: dinov2_vits14\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mStudent model created: edgenext_xx_small\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mCreating distillation module...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mStarting DistillationModule initialization...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mInitializing models...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mFreezing teacher model...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mTeacher model frozen.\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mModels initialized.\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mInitializing loss functions...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mLoss functions initialized: ['scalekd_res4', 'scalekd_res5']\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mDistillationModule initialized.\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mDistillation module created.\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mCreating PyTorch Lightning Trainer...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mardaerendogru2632\u001b[0m (\u001b[33mardaerendogru2632-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/disk0/arda/dinov2_finalize/dinov2/distillation/wandb/run-20250313_154238-k0clhthx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33medgenext_xx_small_dinov2_vits14\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ardaerendogru2632-/distillation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ardaerendogru2632-/distillation/runs/k0clhthx\u001b[0m\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mTrainer created: <lightning.pytorch.trainer.trainer.Trainer object at 0x7deb5c917dc0>\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mDistillationTrainer initialized.\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mStarting training process...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mStarting training from scratch.\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mConfiguring optimizers...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mOptimizer configured: AdamW\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mScheduler configured: CosineAnnealingLR\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mConfiguring optimizers...\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mOptimizer configured: AdamW\n",
      "\u001b[32m[03/13 15:42 dinov2]: \u001b[0mScheduler configured: CosineAnnealingLR\n",
      "\n",
      "  | Name    | Type         | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | student | ModelWrapper | 1.2 M  | train\n",
      "1 | teacher | DINOv2ViT    | 22.1 M | eval \n",
      "2 | losses  | ModuleDict   | 7.9 M  | train\n",
      "-------------------------------------------------\n",
      "9.0 M     Trainable params\n",
      "22.1 M    Non-trainable params\n",
      "31.1 M    Total params\n",
      "124.363   Total estimated model params size (MB)\n",
      "291       Modules in train mode\n",
      "212       Modules in eval mode\n",
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:04<00:00,  1.24it/s, v_num=0, lr=0.001]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 1/3 [00:00<00:00,  3.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 2/3 [00:00<00:00,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.51it/s]\u001b[A\n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:02<00:00,  1.25it/s, v_num=0, lr=0.001]\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 1/3 [00:00<00:00,  2.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 2/3 [00:00<00:00,  2.54it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.21it/s]\u001b[A\n",
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:04<00:00,  1.24it/s, v_num=0, lr=0.001]\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 1/3 [00:00<00:00,  3.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 2/3 [00:00<00:00,  2.74it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.31it/s]\u001b[A\n",
      "Epoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 159/229 [02:10<00:57,  1.22it/s, v_num=0, lr=0.001]\u001b[A"
     ]
    }
   ],
   "source": [
    "!python ./train.py --config /home/arda/dinov2_finalize/dinov2/distillation/config/config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distillation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
